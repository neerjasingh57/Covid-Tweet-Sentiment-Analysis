# -*- coding: utf-8 -*-
"""Covid-19 Tweet Sentiment Analysis and Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gFCgBiGrHAPdjFCeF-UFfBdQzy4kmHR5
"""

from google.colab import drive
import pandas as pd

drive.mount('/content/drive')

#df = pd.read_csv("/content/drive/My Drive/Clusters.csv")

df_dropped = pd.read_csv("/content/drive/My Drive/drop_retweets.csv")

len(df_dropped)

pd.set_option('display.precision',17)
df_dropped.head(5)

df_dropped.info()

df_dropped['ID'] = df_dropped['ID'].apply(int)

df_dropped.head(20)

df = df[0:1000000]

#import pandas as pd

#col_list = ['ID', 'Location', 'Processed_Tweet']
#df = pd.read_csv('Clean.csv')

#import csv so that the whole tweet is imported and no '...'s

len(df)

df_t = df.replace('covid 19','cv', regex=True)
df_t = df_t.replace('coronavirus','cv', regex=True)
df_t = df_t.replace('covid-19','cv', regex=True)
df_t = df_t.replace('corona','cv', regex=True)
df_t = df_t.replace('covid','cv', regex=True)

df_t.Processed_Tweet.head(10)

df_t_small = df_t[0:500000]

#df.replace(['covid', 'covid-19', 'corona', 'covid 19', 'corona virus'], 
                     #['coronavirus', 'coronavirus', 'coronavirus', 'coronavirus', 'coronavirus']) 

df = df.replace({'covid': 'coronavirus', 'covid-19': 'coronavirus', 'corona': 'coronavirus', 'covid 19': 'coronavirus',
            'corona virus': 'coronavirus'}, inplace=True)

# remove duplicates
#df = df[~df.duplicated()]

virginia = df[df.Location == 'va']

virginia_t = virginia.replace('\n','', regex=True)
virginia_t = virginia_t.replace(r"@\S+", "", regex=True)
virginia_t = virginia_t.replace(r"https:\S+", "", regex=True)
print('Removing special characters')
virginia_t = virginia_t.replace('[^A-Za-z0-9 ]+','', regex=True)
print('Characters removed.')

virginia_t.head(5)

texts = virginia.Processed_Tweet
texts_list = texts.to_list()
texts_list[1:5]

len(texts_list)

del df_t

df.Location.unique()

#already cleaned
df_t['Location'] = df_t['Location'].replace(
    ['alabama', 'alaska', 'arizona', 'arkansas', 'california', 
     'colorado', 'connecticut', 'delaware', 'florida', 'georgia', 
     'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 
     'kansas', 'kentucky', 'louisiana', 'maine', 'maryland', 
     'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 
     'montana', 'nebraska', 'nevada', 'new hampshire', 'new jersey', 
     'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio', 
     'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 
     'south dakota', 'tennessee', 'texas', 'utah', 'vermont',
     'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming'],
     ['al', 'ak', 'az', 'ar', 'ca', 
      'co', 'ct', 'de', 'fl', 'ga',
      'hi', 'id', 'il', 'in', 'ia', 
      'ks', 'ky', 'la', 'me', 'md', 
      'ma', 'mi', 'mn', 'ms', 'mo',
      'mt', 'ne', 'nv', 'nh', 'nj',
      'nm', 'ny', 'nc', 'nd', 'oh',
      'ok', 'or', 'pa', 'ri', 'sc',
      'sd', 'tn', 'tx', 'ut', 'vt',
      'va', 'wa', 'wv', 'wi', 'wy'])

#states = ['al', 'ak', 'az', 'ar', 'ca', 
      'co', 'ct', 'de', 'fl', 'ga',
      'hi', 'id', 'il', 'in', 'ia', 
      'ks', 'ky', 'la', 'me', 'md', 
      'ma', 'mi', 'mn', 'ms', 'mo',
      'mt', 'ne', 'nv', 'nh', 'nj',
      'nm', 'ny', 'nc', 'nd', 'oh',
      'ok', 'or', 'pa', 'ri', 'sc',
      'sd', 'tn', 'tx', 'ut', 'vt',
      'va', 'wa', 'wv', 'wi', 'wy']

#df_t_state = df_t[df_t['Location'].isin(states)]

#Sentence Clustering: K-Means

from sklearn.feature_extraction.text import TfidfVectorizer

#Sparse Matrix
vect = TfidfVectorizer(min_df = 600, ngram_range=(1,1))
#should we alter some of these parameters?

titles_vect = vect.fit_transform(df_t['Processed_Tweet'])
titles_vect.shape

titles_vect[1:5]

"""###WORD FREQUENCY"""

#make column into list
#WHOLE DATASET
texts = virginia_t.Processed_Tweet
texts_list = texts.to_list()
texts_list[1:5]

import matplotlib.pyplot as plt
from collections import Counter
import numpy as np

# Exploratory analysis to get a general sense idea of wha

print('Pulling out individual words.')
all_words = []
for line in texts_list:
    words = line.split()
    for word in words:
        all_words.append(word.lower())
print('Unique words extracted: {}'.format(np.unique(all_words)))

word_count = Counter(all_words).most_common(25)
word_count_x = []
word_count_y = []
for word, count in word_count:
    word_count_x.append(word)
    word_count_y.append(count)

plt.figure(figsize=(16,6))
plt.plot(word_count_x, word_count_y, linestyle='-', linewidth=1)
plt.ylabel("Count")
plt.xlabel("Word")
plt.xticks(fontsize='small', rotation=90)
plt.title('Plot of words frequency across Corpus')
plt.show()

#most common words: corona, covid19, coronavirus, virus, people, amp, covid, test, like, time, new
#case, go, death, say, trump, get, pandemic, need, day, know, work, home, think, help

"""###CLUSTERING"""

from sklearn.cluster import KMeans, MiniBatchKMeans
import matplotlib.pyplot as plt

# Run the Kmeans algorithm, printing out the SSE to identify the inflection "elbow"
sse = []
list_k = list(range(1, 15))

for k in list_k:
    km = MiniBatchKMeans(n_clusters=k, batch_size=200, random_state=3) # use for larger datasets, not as accurate
    #km = KMeans(n_clusters=k)
    km.fit(titles_vect)
    sse.append(km.inertia_)

# Plot sse against k
plt.figure(figsize=(14, 14))
plt.plot(list_k, sse, '-o')
plt.xlabel(r'Number of clusters *k*')
plt.ylabel('Sum of squared distance')
plt.show()

#%%time

# cluster the document using KMeans

# step 1 - import the model
from sklearn.cluster import KMeans, MiniBatchKMeans

# step 2 - instantiate the model
km = KMeans(n_clusters=5, random_state=42)

# step 3 - fit the model with data
# clustering is unsupervised so we do not have labels to add during .fit()
km.fit(titles_vect)

# step 4 - predict the cluster of each section_title
df_t['clusters'] = km.predict(titles_vect)

df_small[1:5]

def review_clusters(df, n_clusters):
    for cl_num in range(n_clusters):
        print(cl_num)
        print(df_t[df_t.clusters == cl_num]['Processed_Tweet'].values[0:10])
        print()

review_clusters(df_t, n_clusters=5)

from sklearn.decomposition import TruncatedSVD # to work with sparse matrices
from sklearn.preprocessing import normalize
from sklearn.metrics import pairwise_distances

sk_tsvd = TruncatedSVD(n_components= 2)

y_tsvd = sk_tsvd.fit_transform(titles_vect)

km = KMeans(n_clusters=8, random_state=42)

# fit to TruncSVD values
km.fit(y_tsvd)

# predict clusters
svd_clusters = km.predict(y_tsvd)

# plot results
plt.scatter(y_tsvd[:, 0], y_tsvd[:, 1], c=svd_clusters, s=50, cmap='magma')
plt.show()

import numpy as np

df = df[df['clusters'] == 1]
cluster1 = df.Processed_Tweet
cluster1_list = cluster1.to_list()

#tweets must be in "raw" form

print('Pulling out individual words.')
all_words = []
for line in cluster1_list:
    words = line.split()
    for word in words:
        all_words.append(word.lower())
print('Unique words extracted: {}'.format(np.unique(all_words)))

word_count = Counter(all_words).most_common(15)
word_count_x = []
word_count_y = []
for word, count in word_count:
    word_count_x.append(word)
    word_count_y.append(count)

plt.figure(figsize=(16,6))
plt.plot(word_count_x, word_count_y, linestyle='-', linewidth=1)
plt.ylabel("Count")
plt.xlabel("Word")
plt.xticks(fontsize='small', rotation=90)
plt.title('Plot of words frequency across Corpus')
plt.show()

#cluster0 = coronavirus, trump, covid19, death, pandemic, new, case, say, state, china, new, lockdown, day, time, report
#cluster1 = covid19, case, new, death, pandemic, time, health, patient, corona, help, report, trump, state, day, home

"""###SENTIMENT ANALYSIS"""

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

# create a function to pass our sentences
def sentiment_analyzer_scores(sentence):
    score = analyzer.polarity_scores(sentence)
    #print("{:-<60} {}".format(sentence, str(score)))
    return score

# let's apply to our Amazon review dataset and see how well the data was labeled
# we will specifically pull out the 'compound score
df_dropped['score'] = df_dropped['Processed_Tweet'].apply(lambda review: sentiment_analyzer_scores(review)['compound'])

df_dropped.head(5)

meansID = df_dropped.groupby('ID').mean()

len(meansID)

meansID.head()

meansID['V_Sentiment'] = meansID['score'].apply(lambda c: 'pos' if c >=0.05 else ('neu' if c > -0.05 else 'neg'))

meansID_Location = meansID.groupby('Location').mean()

means.sort_values('score')

df_dropped['V_Sentiment_Specific'] = df_dropped['score'].apply(lambda c: 'very pos' if c >= 0.50 else 
                                                   ('pos' if c > 0.05 else 
                                                   ('neu' if c > -0.05 else 
                                                   ('neg' if c > -0.50 else 'very neg'))))

df_dropped.head()

#add all numbers in score column
#count V_sentiment scores for each neu, neg, and pos

Total = df_dropped['score'].sum()
print (Total)

neu = len(df_dropped[df_dropped['V_Sentiment'] == 'neu'])
pos = len(df_dropped[df_dropped['V_Sentiment'] == 'pos'])
neg = len(df_dropped[df_dropped['V_Sentiment'] == 'neg'])
print('neu:', neu)
print('pos:', pos)
print('neg:', neg)

verypos = len(df_dropped[df_dropped['V_Sentiment_Specific'] == 'very pos'])
pos = len(df_dropped[df_dropped['V_Sentiment_Specific'] == 'pos'])
neu = len(df_dropped[df_dropped['V_Sentiment_Specific'] == 'neu'])
neg = len(df_dropped[df_dropped['V_Sentiment_Specific'] == 'neg'])
veryneg = len(df_dropped[df_dropped['V_Sentiment_Specific'] == 'very neg'])

print('very pos:', verypos)
print('pos:', pos)
print('neu:', neu)
print('neg:', neg)
print('very neg:', veryneg)

df_dropped = pd.get_dummies(df_dropped, columns=['V_Sentiment_Specific'])

means = df_dropped.groupby('Location').mean()

means.sort_values('score')

means.sort_values('V_Sentiment_Specific_very neg')

#states with highest mean of very negative tweets: texas, nevada, mississippi, new jersey and arizona
#

means.score.mean()

counts = df_dropped.groupby('Location').count()
counts.sort_values('score')

import numpy as np
import matplotlib.pyplot as plt

performlist = [verypos, pos, neu, neg, veryneg]

objects = ('very pos', 'pos', 'neu', 'neg', 'very neg')
y_pos = np.arange(len(objects))
performance = performlist

plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('Tweets')
plt.title('Tweet Sentiment Across 50 States')

plt.show()

objects = ('pos', 'neu', 'neg')

performlist2 = [pos, neu, neg]

y_pos = np.arange(len(objects))
performance = performlist2

plt.bar(y_pos, performance, align='center', alpha=0.5)
plt.xticks(y_pos, objects)
plt.ylabel('Tweets')
plt.title('Tweet Sentiment Across 50 States')

plt.show()