# -*- coding: utf-8 -*-
"""Deliv 2 - Singh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jzCbKQfcjkjNzyl5cGuCbPdHExJdK1m8

On the first 10,000 tweets:

-Use SpaCy to conduct lemmatization

-Create a Term-Document Matrix (TDM)

-Develop a TF-IDF from the TDM Matrix

-Generate a TF-IDF scatter plot
"""

import pandas as pd
from google.colab import files

df = pd.read_csv('covid-extract_covid_tweets_sample.csv')

texts_list = df['Full_Text'].tolist()
texts_list = texts_list[0:10000]
len(texts_list)

from sklearn.feature_extraction.text import CountVectorizer

# INSTALLATION
!python -m spacy download es_core_news_sm
import spacy

# create an instance of countvectorizer (sklearn)
vect = CountVectorizer()

# loading spaCy for processing / comparison with sklearn functions
nlp = spacy.load('en')

# We will start the cleaning process of the strings of text or 'documents.'
# We will remove any punctuation and append the strings into their original
# document structure

doc_num = 0

raw_docs = []

# We will leverage SpaCy to pull out the lemmenization of the text
# which will extract the 'unique' words across the documents later on.
for docs in texts_list:
  lemm_words = []
  vec_text = nlp(docs)
  doc_num += 1
  for token in vec_text:
    if not (token.is_punct or token.lemma_ == '-PRON-'):
      print('{:15} | {:15} | {:8} | {:8} |'.format(
          token.text, token.lemma_, token.is_punct, doc_num))
      lemm_words.append(token.lemma_)
  staging_doc = ' '.join(lemm_words)
  raw_docs.append(staging_doc)

print('ORIGINAL TEXT {}'.format(texts_list))
print('CLEANED TEXT {}'.format(raw_docs))

# store the dense matrix
data = vect.transform(raw_docs).toarray()

# store the learned vocabulary
columns = vect.get_feature_names()

# combine the data and columns into a dataframe
pd.DataFrame(data, columns=columns)

#Creating TF-IDF

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer()
tf_idf = pd.DataFrame(tfidf_vect.fit_transform(raw_docs).toarray(), columns=columns)

#TF-IDF Scatterplot

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
])   

X = pipeline.fit_transform(tf_idf).todense()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2).fit(X)
data2D = pca.transform(X)
plt.scatter(data2D[:,0], data2D[:,1])
idf_plot = plt.show()