# -*- coding: utf-8 -*-
"""Deliv 3 - Singh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CER4gAbz_FFeVAYLZepDP1gvukd-XnOp

Deliv 3:

-Conduct the "Elbow Method" to identify optimal K between 1 - 11(Note: this will take a long time)

-Based on K, print out 20 tweets within each cluster; develop a name for each cluster

-Print a graph/distribution of clusters (using TruncatedSVD)

-Develop outline for Final Report
"""

import pandas as pd
from google.colab import files

col_list = ['Full_Text']

df = pd.read_csv('covid-extract_covid_tweets_sample.csv', usecols = col_list)

#import csv so that the whole tweet is imported and no '...'s

# remove duplicates
df = df[~df.Full_Text.duplicated()]

df_small = df[0:500000]
pd.set_option('display.max_colwidth', 300)
df_small.head()

#Sentence Clustering: K-Means

from sklearn.feature_extraction.text import TfidfVectorizer

#Sparse Matrix
vect = TfidfVectorizer(min_df=.25, max_df=.95, norm='l2', stop_words='english', max_features=1000, ngram_range=(1,2))
#should we alter some of these parameters?

titles_vect = vect.fit_transform(df_small['Full_Text'])
titles_vect.shape



from sklearn.cluster import KMeans, MiniBatchKMeans
import matplotlib.pyplot as plt

# Run the Kmeans algorithm, printing out the SSE to identify the inflection "elbow"
sse = []
list_k = list(range(1, 11))

for k in list_k:
    km = MiniBatchKMeans(n_clusters=k, batch_size=200, random_state=3) # use for larger datasets, not as accurate
    #km = KMeans(n_clusters=k)
    km.fit(titles_vect)
    sse.append(km.inertia_)

# Plot sse against k
plt.figure(figsize=(14, 14))
plt.plot(list_k, sse, '-o')
plt.xlabel(r'Number of clusters *k*')
plt.ylabel('Sum of squared distance')
plt.show()

#%%time

# cluster the document using KMeans

# step 1 - import the model
from sklearn.cluster import KMeans, MiniBatchKMeans

# step 2 - instantiate the model
km = KMeans(n_clusters=4, random_state=42)

# step 3 - fit the model with data
# clustering is unsupervised so we do not have labels to add during .fit()
km.fit(titles_vect)

# step 4 - predict the cluster of each section_title
df_small['clusters'] = km.predict(titles_vect)

def review_clusters(df_small, n_clusters):
    for cl_num in range(n_clusters):
        print(cl_num)
        print(df_small[df_small.clusters == cl_num]['Full_Text'].values[0:20])
        print()

review_clusters(df_small, n_clusters=4)

from sklearn.decomposition import TruncatedSVD # to work with sparse matrices
from sklearn.preprocessing import normalize
from sklearn.metrics import pairwise_distances

sk_tsvd = TruncatedSVD(n_components= 2)

y_tsvd = sk_tsvd.fit_transform(titles_vect)

km = KMeans(n_clusters=15, random_state=42)

# fit to TruncSVD values
km.fit(y_tsvd)

# predict clusters
svd_clusters = km.predict(y_tsvd)

# plot results
plt.scatter(y_tsvd[:, 0], y_tsvd[:, 1], c=svd_clusters, s=50, cmap='magma')
plt.show()